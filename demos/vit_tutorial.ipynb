{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7002b686",
   "metadata": {},
   "source": [
    "# Vision Transformer Code Explanation Tutorial\n",
    "\n",
    "This tutorial breaks down the key components and functions from `runner.py` and `network.py`, focusing on practical understanding rather than class structures. You'll learn how each piece works and see demonstrations of their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576ae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "from network import SingleHeadAttention, MultiHeadAttention, TransformerBlock, PatchEmbedding, VisionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cc742",
   "metadata": {},
   "source": [
    "### ViT Block\n",
    "Like before, we have our Transformer block which is assembled out of several multi-head attention modules. Each transformer block first normalizes the input, then applies attention, and then passes the normalized output through a feedforward network. Unlike our NLP transformers, it is typical to perform normalization *prior* to passing through the module\n",
    "\n",
    "**Block Architecture:**\n",
    "```\n",
    "Input → LayerNorm1 → MultiHeadAttention → + → LayerNorm2 → FeedForward → +\n",
    "  ↑                                                                      ↑\n",
    "  └──────────────────── Residual Connection ─────────────────────────────┘\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb1dbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.attention = MultiHeadAttention(hidden_size, hidden_size, hidden_size, num_heads)\n",
    "        self.net = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.GELU(), nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.net(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836caff7",
   "metadata": {},
   "source": [
    "### Patch Embedding\n",
    "What is a \"token\" in an image? We can take the token and break it into \"patches\" - essentially by performing a convolution over the image and downsizing the image\n",
    "\n",
    "**Key Functions:**\n",
    "- **Image Patching**: Divides image into 4×4 patches\n",
    "- **Linear Projection**: Converts patches to high-dimensional vectors\n",
    "- **CLS Token**: Special token for classification\n",
    "- **Position Embeddings**: Learnable position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15508fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, channels, img_size=32, patch_size=4, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.channels = channels\n",
    "        # Each patch is patch_size x patch_size pixels\n",
    "        # Total number of patches is (img_size/patch_size)^2\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(channels, hidden_size, kernel_size=patch_size, stride=patch_size) \n",
    "        # Learnable CLS token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        # Positional embedding for patches + CLS token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, hidden_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] -> [B, num_patches, hidden_size]\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd047c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: torch.Size([1, 3, 32, 32])\n",
      "Patches shape: torch.Size([1, 64, 128])\n",
      "Label: cat\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load CIFAR10 dataset and get one image\n",
    "transform = transforms.ToTensor()\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "image, label = cifar10[0]  # Get first image\n",
    "image = image.unsqueeze(0)  # Add batch dimension [1, 3, 32, 32]\n",
    "\n",
    "patch_embed = PatchEmbedding(\n",
    "    channels=3,\n",
    "    img_size=32,\n",
    "    patch_size=4,\n",
    "    hidden_size=128\n",
    ")\n",
    "\n",
    "# Get patches from the image\n",
    "patches = patch_embed(image)\n",
    "\n",
    "print(f\"Original image shape: {image.shape}\")  # [1, 3, 32, 32]\n",
    "print(f\"Patches shape: {patches.shape}\")  # [1, 64, 128] - 64 \"patches\" of size (representation) 128\n",
    "print(f\"Label: {cifar10.classes[label]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
